import json
import openai
from typing import List, Dict, Callable, Any, Union, Optional, Generator, Tuple

import logging
import os
from src.base_model import BaseLLMModel
# pip install openai==0.28

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
TIMEOUT_STREAMING = 120  # æµå¼å¯¹è¯æ—¶çš„è¶…æ—¶æ—¶é—´
TIMEOUT_ALL = 120        # éæµå¼å¯¹è¯æ—¶çš„è¶…æ—¶æ—¶é—´
ENABLE_STREAMING_OPTION = True  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
HIDE_MY_KEY = True               # æ˜¯å¦åœ¨UIä¸­éšè—APIå¯†é’¥
CONCURRENT_COUNT = 100           # å…è®¸åŒæ—¶ä½¿ç”¨çš„ç”¨æˆ·æ•°é‡
SIM_K = 5
INDEX_QUERY_TEMPRATURE = 1.0
TITLE = "OpenAI ğŸš€"  # æ ¹æ®éœ€è¦æ›¿æ¢
STANDARD_ERROR_MSG = "â˜¹ï¸å‘ç”Ÿäº†é”™è¯¯ï¼š"  # æ ¹æ®éœ€è¦æ›¿æ¢
GENERAL_ERROR_MSG = "è·å–å¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·é‡è¯•"  # æ ¹æ®éœ€è¦æ›¿æ¢
INITIAL_SYSTEM_PROMPT = "You are a helpful assistant."

def safe_unicode_decode(bytes_obj: bytes) -> str:
    """
    å®‰å…¨åœ°è§£ç  Unicode å­—ç¬¦ä¸²ï¼Œå¤„ç†å¯èƒ½çš„ç¼–ç é”™è¯¯ã€‚
    """
    try:
        return bytes_obj.decode("utf-8")
    except UnicodeDecodeError:
        return bytes_obj.decode("utf-8", errors="replace")

def openai_complete_if_cache(
    model: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: Optional[List[Dict[str, str]]] = None,
    base_url: Optional[str] = None,
    api_key: Optional[str] = None,
    stream: bool = False,
    **kwargs,
) -> Union[str, Generator[str, None, None]]:
    """
    åŒæ­¥ç‰ˆæœ¬çš„ OpenAI æ¨¡å‹è¯·æ±‚å‡½æ•°ï¼Œæ”¯æŒç¼“å­˜ã€‚

    Args:
        model (str): æ¨¡å‹åç§°ã€‚
        prompt (str): ç”¨æˆ·è¾“å…¥çš„æç¤ºã€‚
        system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºã€‚
        history_messages (Optional[List[Dict[str, str]]]): å¯¹è¯å†å²ã€‚
        base_url (Optional[str]): OpenAI API çš„åŸºç¡€ URLï¼ˆç”¨äºä»£ç†ç­‰ï¼‰ã€‚
        api_key (Optional[str]): OpenAI API å¯†é’¥ã€‚
        stream (bool): æ˜¯å¦å¯ç”¨æµå¼å“åº”ã€‚
        **kwargs: å…¶ä»–å¯é€‰å‚æ•°ã€‚

    Returns:
        Union[str, Generator[str, None, None]]: å®Œæ•´å“åº”å†…å®¹æˆ–ç”Ÿæˆå™¨ã€‚
    """
    if api_key:
        openai.api_key = api_key
    if base_url:
        openai.api_base = base_url

    history_messages = history_messages or []
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})

    # æ·»åŠ æ—¥å¿—è¾“å‡º
    logger.debug("===== Query Input to OpenAI =====")
    logger.debug(f"Query: {prompt}")
    logger.debug(f"System prompt: {system_prompt}")
    logger.debug("Full context:")
    for msg in messages:
        logger.debug(f"{msg['role']}: {msg['content']}")
    # bm = BaseLLMModel
    # print("ollama_demo")
    try:
        if stream:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                stream=True,
                **kwargs,
            )
        else:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                stream=False,
                **kwargs,
            )
    except Exception as e:
        logger.error(f"OpenAI è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return None

    if stream:
        def stream_generator():
            try:
                for chunk in response:
                    content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                    if content:
                        if r"\u" in content:
                            content = safe_unicode_decode(content.encode("utf-8"))
                        yield content
            except Exception as e:
                logger.error(f"æµå¼å“åº”æ—¶å‡ºé”™: {e}")
                yield STANDARD_ERROR_MSG + GENERAL_ERROR_MSG

        return stream_generator()
    else:
        content = response.get("choices", [{}])[0].get("message", {}).get("content", "")
        if r"\u" in content:
            content = safe_unicode_decode(content.encode("utf-8"))
        return content

class OllamaAIClient(BaseLLMModel):
    def __init__(
        self,
        model_name: str,
        api_key: str,
        system_prompt: str = INITIAL_SYSTEM_PROMPT,
        temperature: float = 1.0,
        top_p: float = 1.0,
        user_name: str = "",
        base_url: Optional[str] = None,
        timeout: Optional[int] = None,  # OpenAI Python SDK handles timeout internally
    ) -> None:
        super().__init__(
            model_name=model_name,
            temperature=temperature,
            top_p=top_p,
            system_prompt=system_prompt,
            user=user_name,
        )
        self.api_key = api_key
        self.base_url =  "http://127.0.0.1:12345/v1"
        self.timeout = timeout
        self.need_api_key = False  # æ ¹æ®å®é™…æƒ…å†µè®¾ç½®

    def get_answer_stream_iter(self) -> Generator[str, None, None]:
        """
        è·å–æµå¼å›ç­”çš„ç”Ÿæˆå™¨ã€‚
        """
        temp_history = self.history[:-1] # æœ€åä¼šä¼ å…¥ä¸¤ä¸ª, æŠŠ history æœ«ä½çš„å…ˆæ¸…æ‰
        if not self.api_key:
            raise ValueError("API key is not set")
        response = openai_complete_if_cache(
            model=self.model_name,
            prompt=self.get_current_prompt(),
            system_prompt=self.system_prompt,
            history_messages=temp_history,
            base_url=self.base_url,
            api_key=self.api_key,
            stream=True,
            temperature=self.temperature,
            top_p=self.top_p,
            # **self.additional_kwargs(),  # å¦‚æœæœ‰å…¶ä»–å‚æ•°éœ€è¦ä¼ é€’
        )
        if response is None:
            logger.error("OpenAI API returned None response")
            yield STANDARD_ERROR_MSG
            return

    
        # response = response.split("ã€‚")
        # import time

        try:
            prev_partial_text: str = ""
            for partial_text in response:
                # if partial_text.strip():
                    # print(partial_text)
                if partial_text is None or not isinstance(partial_text, str):
                    logger.warning(f"Unexpected response type: {type(partial_text)}")
                    continue
                prev_partial_text += partial_text
                # time.sleep(0.3)
                yield prev_partial_text
        except Exception as e:
            logger.error(f"Error in get_answer_stream_iter: {str(e)}", exc_info=True)
            yield STANDARD_ERROR_MSG + GENERAL_ERROR_MSG


    def get_answer_at_once(self) -> Tuple[str, int]:
        """
        ä¸€æ¬¡æ€§è·å–å®Œæ•´å›ç­”ã€‚
        """
        if not self.api_key:
            raise ValueError("API key is not set")
        temp_history = self.history[:-1]
        response = openai_complete_if_cache(
            model=self.model_name,
            prompt=self.get_current_prompt(),
            system_prompt=self.system_prompt,
            history_messages=temp_history,
            base_url=self.base_url,
            api_key=self.api_key,
            stream=False,
            temperature=self.temperature,
            top_p=self.top_p,
            # **self.additional_kwargs(),  # å¦‚æœæœ‰å…¶ä»–å‚æ•°éœ€è¦ä¼ é€’
        )
        if response:
            try:
                # å‡è®¾æ²¡æœ‰æä¾› token è®¡æ•°ä¿¡æ¯ï¼Œè¿™é‡Œè®¾ä¸º 0
                content = response
                total_token_count = 0
                return content, total_token_count
            except Exception as e:
                logger.error(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
                raise Exception(STANDARD_ERROR_MSG + GENERAL_ERROR_MSG)
        else:
            raise Exception(STANDARD_ERROR_MSG + GENERAL_ERROR_MSG)

    def get_current_prompt(self) -> str:
        """
        è·å–å½“å‰çš„æç¤ºå†…å®¹ï¼Œä»…åŒ…å«æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
        """
        latest_user_message = self.get_latest_user_message()
        # print(latest_user_message)
        if latest_user_message:
            return latest_user_message
        return ""

    def get_latest_user_message(self) -> Optional[str]:
        """
        è·å–å†å²è®°å½•ä¸­æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
        """
        user_messages = [msg["content"] for msg in self.history if msg["role"] == "user"]
        if user_messages:
            return user_messages[-1]
        return None

    # def additional_kwargs(self) -> Dict[str, Any]:
    #     """
    #     è¿”å›é¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œç”¨äºä¼ é€’ç»™ OpenAI APIã€‚
    #     æ ¹æ®éœ€è¦å®ç°æˆ–æ‰©å±•æ­¤æ–¹æ³•ã€‚
    #     """
    #     return {}
    # def auto_load(self):
    #     self.history_file_path = new_auto_history_filename(self.user_name)
    #     return self.load_chat_history()
    
# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    client = OpenAIClient(
        model_name="gpt-4",
        api_key="your-openai-api-key",
        base_url=None,  # å¦‚æœä½¿ç”¨ä»£ç†ï¼Œå¯ä»¥è®¾ç½®ä¸ºä»£ç†åœ°å€
        temperature=0.7,
        top_p=0.9
    )

    # æ·»åŠ å¯¹è¯å†å²ï¼ˆç¤ºä¾‹ï¼‰
    client.history = [
        {"role": "system", "content": client.system_prompt},
        {"role": "user", "content": "ä½ å¥½ï¼Œèƒ½å¸®æˆ‘å†™ä¸€é¦–è¯—å—ï¼Ÿ"}
    ]

    # è·å–ä¸€æ¬¡æ€§å›ç­”
    try:
        answer, tokens = client.get_answer_at_once()
        print(f"Answer: {answer}\nTokens used: {tokens}")
    except Exception as e:
        print(str(e))

    # è·å–æµå¼å›ç­”
    try:
        print("Streamed Answer: ", end='')
        for partial in client.get_answer_stream_iter():
            print(partial, end='', flush=True)
        print()  # æ¢è¡Œ
    except Exception as e:
        print(str(e))
